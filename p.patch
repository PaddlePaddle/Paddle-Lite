diff --git a/cmake/configure.cmake b/cmake/configure.cmake
index 767b082..a8065c5 100644
--- a/cmake/configure.cmake
+++ b/cmake/configure.cmake
@@ -189,10 +189,6 @@ if (LITE_WITH_HUAWEI_ASCEND_NPU)
 add_definitions("-DLITE_WITH_HUAWEI_ASCEND_NPU")
 endif()
 
-if (LITE_WITH_MMA)
-add_definitions("-DLITE_WITH_MMA")
-endif()
-
 if (LITE_WITH_PROFILE)
     add_definitions("-DLITE_WITH_PROFILE")
 endif()
diff --git a/cmake/lite.cmake b/cmake/lite.cmake
index 1339e7d..2be0175 100644
--- a/cmake/lite.cmake
+++ b/cmake/lite.cmake
@@ -22,7 +22,7 @@ endfunction()
 function (lite_deps TARGET)
   set(options "")
   set(oneValueArgs "")
-  set(multiValueArgs DEPS X86_DEPS CUDA_DEPS ARM_DEPS PROFILE_DEPS LIGHT_DEPS HVY_DEPS CL_DEPS FPGA_DEPS BM_DEPS RKNPU_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS IMAGINATION_NNA_DEPS APU_DEPS CV_DEPS MMA_DEPS ARGS)
+  set(multiValueArgs DEPS X86_DEPS CUDA_DEPS ARM_DEPS PROFILE_DEPS LIGHT_DEPS HVY_DEPS CL_DEPS FPGA_DEPS BM_DEPS RKNPU_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS IMAGINATION_NNA_DEPS APU_DEPS CV_DEPS ARGS)
   cmake_parse_arguments(lite_deps "${options}" "${oneValueArgs}" "${multiValueArgs}" ${ARGN})
 
   set(deps ${lite_deps_DEPS})
@@ -129,12 +129,6 @@ function (lite_deps TARGET)
       set(deps ${deps} ${var})
     endforeach(var)
   endif()
-  
-  if (LITE_WITH_MMA)
-    foreach(var ${lite_deps_MMA_DEPS})
-      set(deps ${deps} ${var})
-    endforeach(var)
-  endif()
 
   set(${TARGET} ${deps} PARENT_SCOPE)
 endfunction()
@@ -161,7 +155,7 @@ file(WRITE ${offline_lib_registry_file} "") # clean
 function(lite_cc_library TARGET)
     set(options SHARED shared STATIC static MODULE module)
     set(oneValueArgs "")
-    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS RKNPU_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS CV_DEPS MMA_DEPS PROFILE_DEPS LIGHT_DEPS
+    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS RKNPU_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS CV_DEPS PROFILE_DEPS LIGHT_DEPS
       HVY_DEPS EXCLUDE_COMPILE_DEPS ARGS)
     cmake_parse_arguments(args "${options}" "${oneValueArgs}" "${multiValueArgs}" ${ARGN})
 
@@ -185,7 +179,7 @@ function(lite_cc_library TARGET)
             HVY_DEPS ${args_HVY_DEPS}
             MLU_DEPS ${args_MLU_DEPS}
             HUAWEI_ASCEND_NPU_DEPS ${args_HUAWEI_ASCEND_NPU_DEPS}
-            MMA_DEPS ${args_MMA_DEPS})
+            )
 
     if (args_SHARED OR ARGS_shared)
         cc_library(${TARGET} SRCS ${args_SRCS} DEPS ${deps} SHARED)
@@ -213,7 +207,7 @@ function(lite_cc_binary TARGET)
         set(options " -g ")
     endif()
     set(oneValueArgs "")
-    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS RKNPU NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS MMA_DEPS PROFILE_DEPS
+    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS RKNPU NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS PROFILE_DEPS
       LIGHT_DEPS HVY_DEPS EXCLUDE_COMPILE_DEPS CV_DEPS ARGS)
     cmake_parse_arguments(args "${options}" "${oneValueArgs}" "${multiValueArgs}" ${ARGN})
 
@@ -237,7 +231,6 @@ function(lite_cc_binary TARGET)
             CV_DEPS ${CV_DEPS}
             MLU_DEPS ${args_MLU_DEPS}
             HUAWEI_ASCEND_NPU_DEPS ${args_HUAWEI_ASCEND_NPU_DEPS}
-            MMA_DEPS ${args_MMA_DEPS}
             )
     cc_binary(${TARGET} SRCS ${args_SRCS} DEPS ${deps})
     if(NOT WIN32)
@@ -269,7 +262,7 @@ function(lite_cc_test TARGET)
     endif()
     set(options "")
     set(oneValueArgs "")
-    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS RKNPU_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS MMA_DEPS PROFILE_DEPS
+    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS RKNPU_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS PROFILE_DEPS
         LIGHT_DEPS HVY_DEPS EXCLUDE_COMPILE_DEPS CV_DEPS
         ARGS
         COMPILE_LEVEL # (basic|extra)
@@ -301,7 +294,7 @@ function(lite_cc_test TARGET)
               CV_DEPS ${args_CV_DEPS}
               MLU_DEPS ${args_MLU_DEPS}
               HUAWEI_ASCEND_NPU_DEPS ${args_HUAWEI_ASCEND_NPU_DEPS}
-              MMA_DEPS ${args_MMA_DEPS})
+              )
     _lite_cc_test(${TARGET} SRCS ${args_SRCS} DEPS ${deps} ARGS ${args_ARGS})
     # strip binary target to reduce size
     if(NOT "${CMAKE_BUILD_TYPE}" STREQUAL "Debug")
@@ -334,7 +327,6 @@ set(bm_kernels CACHE INTERNAL "bm kernels")
 set(imagination_nna_kernels CACHE INTERNAL "imagination_nna kernels")
 set(rknpu_kernels CACHE INTERNAL "rknpu kernels")
 set(opencl_kernels CACHE INTERNAL "opencl kernels")
-set(mma_kernels CACHE INTERNAL "mma kernels")
 set(host_kernels CACHE INTERNAL "host kernels")
 
 set(kernels_src_list "${CMAKE_BINARY_DIR}/kernels_src_list.txt")
@@ -354,7 +346,7 @@ endif()
 function(add_kernel TARGET device level)
     set(options "")
     set(oneValueArgs "")
-    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS RKNPU_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS MMA_DEPS PROFILE_DEPS
+    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS RKNPU_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS PROFILE_DEPS
         LIGHT_DEPS HVY_DEPS EXCLUDE_COMPILE_DEPS
         ARGS)
     cmake_parse_arguments(args "${options}" "${oneValueArgs}" "${multiValueArgs}" ${ARGN})
@@ -509,16 +501,6 @@ function(add_kernel TARGET device level)
         nv_library(${TARGET} SRCS ${args_SRCS} DEPS ${args_DEPS})
         return()
     endif()
-    
-    if ("${device}" STREQUAL "MMA")
-        if (NOT LITE_WITH_MMA)
-            foreach(src ${args_SRCS})
-                file(APPEND ${fake_kernels_src_list} "${CMAKE_CURRENT_SOURCE_DIR}/${src}\n")
-            endforeach()
-            return()
-        endif()
-        set(mma_kernels "${mma_kernels};${TARGET}" CACHE INTERNAL "")
-    endif()
 
     # the source list will collect for paddle_use_kernel.h code generation.
     foreach(src ${args_SRCS})
@@ -540,7 +522,6 @@ function(add_kernel TARGET device level)
               MLU_DEPS ${args_MLU_DEPS}
               IMAGINATION_NNA_DEPS ${args_IMAGINATION_NNA_DEPS}
               HUAWEI_ASCEND_NPU_DEPS ${args_HUAWEI_ASCEND_NPU_DEPS}
-              MMA_DEPS ${args_MMA_DEPS}
               PROFILE_DEPS ${args_PROFILE_DEPS}
               LIGHT_DEPS ${args_LIGHT_DEPS}
               HVY_DEPS ${args_HVY_DEPS}
@@ -559,7 +540,7 @@ endif()
 function(add_operator TARGET level)
     set(options "")
     set(oneValueArgs "")
-    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS MMA_DEPS PROFILE_DEPS
+    set(multiValueArgs SRCS DEPS X86_DEPS CUDA_DEPS CL_DEPS ARM_DEPS FPGA_DEPS BM_DEPS IMAGINATION_NNA_DEPS NPU_DEPS XPU_DEPS MLU_DEPS HUAWEI_ASCEND_NPU_DEPS APU_DEPS PROFILE_DEPS
         LIGHT_DEPS HVY_DEPS EXCLUDE_COMPILE_DEPS
         ARGS)
     cmake_parse_arguments(args "${options}" "${oneValueArgs}" "${multiValueArgs}" ${ARGN})
@@ -599,10 +580,10 @@ function(add_operator TARGET level)
               MLU_DEPS ${args_MLU_DEPS}
               IMAGINATION_NNA_DEPS ${args_IMAGINATION_NNA_DEPS}
               HUAWEI_ASCEND_NPU_DEPS ${args_HUAWEI_ASCEND_NPU_DEPS}
-              MMA_DEPS ${args_MMA_DEPS}
               PROFILE_DEPS ${args_PROFILE_DEPS}
               LIGHT_DEPS ${args_LIGHT_DEPS}
-              HVY_DEPS ${args_HVY_DEPS})
+              HVY_DEPS ${args_HVY_DEPS}
+      )
 endfunction()
 
 #only for windows 
diff --git a/docs/demo_guides/mma.md b/docs/demo_guides/mma.md
deleted file mode 100644
index ea044d6..0000000
--- a/docs/demo_guides/mma.md
+++ /dev/null
@@ -1,107 +0,0 @@
-# PaddleLite使用MMA预测部署
-
-Paddle Lite支持基于ARM+MMA的模型预测，MMA为矩阵运算加速器，例如，FPGA，DSP，GPU等协处理硬件运算单元。
-
-Lite基于MMA运行模型需要相应的设备驱动，以及对应的API接口。
-
-## Lite实现MMA简介
-
-Lite支持MMA作为后端硬件进行模型推理，其主要特性如下：
-
-- Lite中MMA的kernel均以FP32、NCHW的格式作为输入输出格式
-
-- 对于MMA暂不支持的kernel，均会切回ARM端运行，实现ARM+MMA混合布署运行
-
-## 支持现状
-- [Cyclone V](https://www.intel.cn/content/dam/altera-www/global/en_US/pdfs/literature/hb/cyclone-v/cv_51002.pdf)
-
-### 已支持（或部分支持）的Paddle算子
-
-- relu/relu6/leakyrelu
-- conv2d
-- depthwise_conv2d
-
-### 已支持的Paddle模型
-
-- [SSD_MobileNet_V1](https://paddlemodels.bj.bcebos.com/object_detection/ssd_mobilenet_v1_coco_pretrained.tar)
-
-## 编译
-
-需要提前准备带有mmadrv.ko的MMA开发板C5MB/C5TB和Lite代码，Lite的交叉编译器采用arm-gcc-5.4.1
-
-CMAKE编译选项：
-
-- 设置`LITE_WITH_MMA=ON`和`LITE_WITH_ARM=ON`
-
-其他编译选项与ARM编译相同，可以参考[“Paddle Lite在Docker下的ARM编译”](../source_compile/compile_linux)。
-
-示例如下：
-```shell
-    cmake .. \
-        -DWITH_GPU=OFF \
-        -DWITH_MKL=OFF \
-        -DWITH_LITE=ON \
-        -DLITE_WITH_CUDA=OFF \
-        -DLITE_WITH_X86=OFF \
-        -DLITE_WITH_ARM=ON \
-        -DLITE_WITH_OPENMP=ON   \
-        -DLITE_WITH_LIGHT_WEIGHT_FRAMEWORK=ON \
-        -DWITH_TESTING=OFF \
-        -DLITE_WITH_MMA=ON \
-        -DARM_TARGET_OS=armlinux 
-    make publish_inference -j2
-```
-Lite提供FPGA编译脚本，位于lite/tools/build_mma.sh，在Lite根目录执行该脚本即可编译
-
-## 运行示例
-
-- **运行文件准备**
-
-下面以SSD模型为例，介绍如何使用C5MB/C5TB开发板实现模型运行
-
-```bash
-#打开串口调试工具，如Putty或SecureCRT，选择对应的调试串口，并设置串口属性，
-#波特率：115200，数据位：8，停止位：1，奇偶校验：无[主机上执行]
-#上电C5MB开发板，并在串口调试工具中登录
-awcloud login: root
-Password: #密码：Awcloud@123
-#进入/opt目录[开发板执行]
-cd /opt
-#在运行模型前需要加载FPGA驱动[开发板执行]
-insmod driver/mmadrv.ko
-```
-
-- **使用FPGA进行模型预测**
-
-```bash
-#以下命令均在开发板上运行，在开发板上已经部署了对应的输入图片，模型，驱动程序，执行程序等
-#运行SSD测试程序，输入图片为/opt/images/dog.jpg，输出图片为/opt/dog_result.jpg
-./run_ssd.sh
-```
-
-## 如何在Code中使用
-
-在Lite中使用MMA与ARM相似，具体的区别如下：
-
-- 由于MMA运行模式为FP32精度、NCHW布局，所以需要修改相应的`valid_place`
-
-代码示例：
-```cpp
-lite::Predictor predictor;
-std::vector<Place> valid_places(
-      {Place{TARGET(kMMA), PRECISION(kFloat), DATALAYOUT(kNCHW)},Place{TARGET(kARM)});
-
-predictor.Build(model_dir, "", "", valid_places);
-
-auto* input_tensor = predictor.GetInput(0);
-input_tensor->Resize(DDim(std::vector<DDim::value_type>({1, 3, 224, 224})));
-auto* data = input_tensor->mutable_data<float>();
-auto item_size = input_tensor->dims().production();
-//假设设置输入数据全为1
-for (int i = 0; i < item_size; i++) {
-  data[i] = 1;
-}
-
-predictor.Run();
-auto* out = predictor.GetOutput(0);
-```
diff --git a/lite/api/paddle_place.cc b/lite/api/paddle_place.cc
index 6ad1489..a04d632 100644
--- a/lite/api/paddle_place.cc
+++ b/lite/api/paddle_place.cc
@@ -82,8 +82,7 @@ const std::string& TargetToStr(TargetType target) {
                                               "rknpu",
                                               "apu",
                                               "huawei_ascend_npu",
-                                              "imagination_nna",
-                                              "mma"};
+                                              "imagination_nna"};
   auto x = static_cast<int>(target);
   CHECK_LT(x, static_cast<int>(TARGET(NUM)));
   return target2string[x];
@@ -130,8 +129,7 @@ const std::string& TargetRepr(TargetType target) {
                                               "kRKNPU",
                                               "kAPU",
                                               "kHuaweiAscendNPU",
-                                              "kImaginationNNA",
-                                              "kMMA"};
+                                              "kImaginationNNA"};
   auto x = static_cast<int>(target);
   CHECK_LT(x, static_cast<int>(TARGET(NUM)));
   return target2string[x];
@@ -192,8 +190,7 @@ std::set<TargetType> ExpandValidTargets(TargetType target) {
                                                TARGET(kRKNPU),
                                                TARGET(kFPGA),
                                                TARGET(kHuaweiAscendNPU),
-                                               TARGET(kImaginationNNA)},
-                                               TARGET(kMMA)});
+                                               TARGET(kImaginationNNA)});
   if (target == TARGET(kAny)) {
     return valid_set;
   }
diff --git a/lite/api/paddle_place.h b/lite/api/paddle_place.h
index ff0e60f..f0563f6 100644
--- a/lite/api/paddle_place.h
+++ b/lite/api/paddle_place.h
@@ -59,8 +59,7 @@ enum class TargetType : int {
   kAPU = 13,
   kHuaweiAscendNPU = 14,
   kImaginationNNA = 15,
-  kMMA = 16,
-  NUM = 17,  // number of fields.
+  NUM = 16,  // number of fields.
 };
 enum class PrecisionType : int {
   kUnk = 0,
diff --git a/lite/backends/CMakeLists.txt b/lite/backends/CMakeLists.txt
index 2ee466e..0ebf133 100644
--- a/lite/backends/CMakeLists.txt
+++ b/lite/backends/CMakeLists.txt
@@ -12,4 +12,3 @@ add_subdirectory(apu)
 add_subdirectory(rknpu)
 add_subdirectory(huawei_ascend_npu)
 add_subdirectory(imagination_nna)
-add_subdirectory(mma)
diff --git a/lite/backends/mma/CMakeLists.txt b/lite/backends/mma/CMakeLists.txt
deleted file mode 100644
index addee10..0000000
--- a/lite/backends/mma/CMakeLists.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-if (NOT LITE_WITH_MMA)
-    return()
-endif()
-
-set(LITE_MMA_PATH "${PADDLE_SOURCE_DIR}/lite/backends/mma")
-set(LITE_MMA_LLDRV_PATH "${PADDLE_SOURCE_DIR}/lite/backends/mma/lldrv")
-
-message("mma_path ${LITE_MMA_PATH}")
-file(GLOB MMA_CPP "${LITE_MMA_PATH}/*.cpp")
-file(GLOB LLDRV_CPP "${LITE_MMA_LLDRV_PATH}/*.cpp")
-message("mma cpp: ${MMA_CPP}")
-set(MMA_ALL_CPP "")
-FOREACH(FILE_PATH ${LLDRV_CPP})
-    STRING(REGEX REPLACE ".+/(.+\\..*)" "\\1" FILE_NAME ${FILE_PATH})
-    list(APPEND MMA_ALL_CPP lldrv/${FILE_NAME})
-ENDFOREACH(FILE_PATH)
-FOREACH(FILE_PATH ${MMA_CPP})
-    STRING(REGEX REPLACE ".+/(.+\\..*)" "\\1" FILE_NAME ${FILE_PATH})
-    list(APPEND MMA_ALL_CPP ${FILE_NAME})
-ENDFOREACH(FILE_PATH)
-message("mma src: ${MMA_ALL_CPP}")
-cc_library(kernel_mma SRCS ${MMA_ALL_CPP})
-cc_library(mma_target_wrapper SRCS target_wrapper.cpp DEPS kernel_mma)
-
diff --git a/lite/backends/mma/lldrv/mmadrv.cpp b/lite/backends/mma/lldrv/mmadrv.cpp
deleted file mode 100644
index 881ce05..0000000
--- a/lite/backends/mma/lldrv/mmadrv.cpp
+++ /dev/null
@@ -1,261 +0,0 @@
-/* Copyright (c) 2020 AWCloud. All Rights Reserved.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License. */
-
-#include <stdio.h>
-#include <fcntl.h>
-#include <sys/ioctl.h>
-#include <sys/mman.h>
-#include <sys/stat.h>
-#include <sys/types.h>
-#include <unistd.h>
-#include <algorithm>
-#include <cstring>
-#include <map>
-#include <utility>
-
-#include "lite/backends/mma/lldrv/mmadrv.h"
-
-namespace paddle {
-namespace lite {
-namespace mma {
-
-// FD of mma
-static int mma_fd = -1;
-
-// Memory blocks
-static struct mma_memblk_s mb, ms, mi, mk, mo;
-
-//---------------------------------------------------------------------------
-
-int mma_open() 
-{
-	if (mma_fd<0) 
-	{
-		mma_fd = open("/dev/mmadrv0", O_RDWR);
-		if (mma_fd<0)
-		{
-			return -1;
-		}
-		memset(&mb, 0, sizeof(mb));
-		memset(&ms, 0, sizeof(ms));
-		memset(&mi, 0, sizeof(mi));
-		memset(&mk, 0, sizeof(mk));
-		memset(&mo, 0, sizeof(mo));
-	}
-
-	return 0;
-}
-
-void mma_close() 
-{
-	if (mma_fd<0) 
-		return;
-	
-	if (mb.addr)
-	{
-		free(mb.addr);
-	}
-	if (ms.addr)
-	{
-		free(ms.addr);
-	}
-	if (mi.addr)
-	{
-		free(mi.addr);
-	}
-	if (mk.addr)
-	{
-		free(mk.addr);
-	}
-	if (mo.addr)
-	{
-		free(mo.addr);
-	}
-	close(mma_fd);
-	mma_fd = -1;
-}
-
-void mma_reset(struct mma_reset_s& args)
-{
-	// TODO
-}
-
-//---------------------------------------------------------------------------
-
-// memory management;
-void *mma_malloc(size_t size) 
-{
-	return malloc(size);
-}
-
-void mma_free(void *ptr) 
-{
-	free(ptr);
-}
-
-void* mma_mbias(size_t size)
-{ 
-	if (mb.addr)
-	{
-		if (mb.size>=size)
-		{
-			return mb.addr;
-		}
-		free(mb.addr);
-	}
-	mb.addr = malloc(size);
-	if (mb.addr)
-	{
-		mb.size = size;
-	}
-	
-	return mb.addr;
-}
-
-void* mma_mscale(size_t size)
-{ 
-	if (ms.addr)
-	{
-		if (ms.size>=size)
-		{
-			return ms.addr;
-		}
-		free(ms.addr);
-	}
-	ms.addr = malloc(size);
-	if (ms.addr)
-	{
-		ms.size = size;
-	}
-	
-	return ms.addr;
-}
-
-void* mma_minput(size_t size)
-{ 
-	if (mi.addr)
-	{
-		if (mi.size>=size)
-		{
-			return mi.addr;
-		}
-		free(mi.addr);
-	}
-	mi.addr = malloc(size);
-	if (mi.addr)
-	{
-		mi.size = size;
-	}
-	
-	return mi.addr;
-}
-
-void* mma_mkernel(size_t size)
-{ 
-	if (mk.addr)
-	{
-		if (mk.size>=size)
-		{
-			return mk.addr;
-		}
-		free(mk.addr);
-	}
-	mk.addr = malloc(size);
-	if (mk.addr)
-	{
-		mk.size = size;
-	}
-	
-	return mk.addr;
-}
-
-void* mma_moutput(size_t size)
-{ 
-	if (mo.addr)
-	{
-		if (mo.size>=size)
-		{
-			return mo.addr;
-		}
-		free(mo.addr);
-	}
-	mo.addr = malloc(size);
-	if (mo.addr)
-	{
-		mo.size = size;
-	}
-	
-	return mo.addr;
-}
-
-void mma_copy(void *dst, void *src, int size) 
-{ 
-	memcpy(dst, src, size); 
-}
-
-int mma_flush(void *addr, size_t size) 
-{
-	// TODO
-	return -1;
-}
-
-int mma_invalidate(void *addr, size_t size) 
-{
-	// TODO
-	return -1;
-}
-
-//---------------------------------------------------------------------------
-
-int mma_info(struct mma_info_s& args)
-{
-	int cmd = MMA_IOCTL_MAKE(MMA_CMD_INFO);
-	
-	if (mma_open()) return -1;
-	
-	return ioctl(mma_fd, cmd, &args);
-}
-
-int mma_conv(struct mma_conv_s& args) 
-{
-	int cmd = MMA_IOCTL_MAKE(MMA_CMD_CONV);
-	
-	if (mma_open()) return -1;
-	
-	return ioctl(mma_fd, cmd, &args);
-}
-
-int mma_pooling(struct mma_pool_s& args) 
-{
-	int cmd = MMA_IOCTL_MAKE(MMA_CMD_POOL);
-	
-	if (mma_open()) return -1;
-	
-	return ioctl(mma_fd, cmd, &args);
-}
-
-int mma_fullconnect(struct mma_fcon_s& args)
-{
-	int cmd = MMA_IOCTL_MAKE(MMA_CMD_FCON);
-	
-	if (mma_open()) return -1;
-	
-	return ioctl(mma_fd, cmd, &args);
-}
-
-//---------------------------------------------------------------------------
-
-}  // namespace mma
-}  // namespace lite
-}  // namespace paddle
diff --git a/lite/backends/mma/lldrv/mmadrv.h b/lite/backends/mma/lldrv/mmadrv.h
deleted file mode 100644
index 3a46c38..0000000
--- a/lite/backends/mma/lldrv/mmadrv.h
+++ /dev/null
@@ -1,185 +0,0 @@
-/* Copyright (c) 2020 AWCloud. All Rights Reserved.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License. */
-
-#ifndef _LLDRV_MMA_H_
-#define _LLDRV_MMA_H_
-
-#pragma once
-
-#include <stdint.h>
-#include <cstddef>
-#include <iostream>
-#include <limits>
-
-namespace paddle {
-namespace lite {
-namespace mma {
-
-// Activation type
-enum mma_act_e {
-	ACT_NONE = 0,
-	ACT_RELU = 1,
-};
-
-// Device information
-struct mma_info_s {
-	uint32_t ver; // Version, 00.00.0000
-};
-
-struct mma_reset_s {
-	uint32_t val; // reset command, N/A
-};
-
-// Memory copy
-struct mma_mcopy_s {
-	void*  src; // source address
-	void*  dst; // destination adddress
-	size_t size; // size in bytes
-};
-
-// Memory block
-struct mma_memblk_s {
-	void * addr; // base address
-	size_t size; // size in bytes
-};
-
-// Kernel
-struct mma_kernel_s {
-	uint32_t kw; // width
-	uint32_t kh; // height
-	uint32_t ws; // width stride(s)
-	uint32_t hs; // height stride(s)
-};
-
-// Input parameters, nchw
-struct mma_input_s {
-	uint32_t in; // nbr of batch {1}
-	uint32_t ic; // nbr of channels {1}
-	uint32_t iw; // width
-	uint32_t ih; // height
-	uint32_t pl; // padding x in bytes {0}
-	uint32_t pr; // padding x in bytes {0}
-	uint32_t pt; // padding y in bytes {0}
-	uint32_t pb; // padding y in bytes {0}
-	uint32_t dx; // dilation for x {1}
-	uint32_t dy; // dilation for y {1}
-};
-
-// Output parameters, nchw
-struct mma_output_s {
-	uint32_t on; // nbr of batch {1}
-	uint32_t oc; // nbr of channels {1}
-	uint32_t ow; // width
-	uint32_t oh; // height
-};
-
-// Basic convolution
-struct mma_conv_s {
-	uint32_t            at; // activation type {0}, None=0, RELU=1
-	uint32_t            ng; // nbr of groups {1}
-	int8_t *            ia; // input address, INT8[N,Ci,Hi,Wi]
-	int8_t *            ka; // kernel address, INT32[Co,Ci,Hk,Wk]
-	int32_t*            ba; // bias address, INT32[Co,1]
-	int32_t*            oa; // output address, INT32[N,Co,Ho,Wo]
-	struct mma_input_s  i; // input
-	struct mma_kernel_s k; // kernel
-	struct mma_output_s o; // output
-};
-
-// Pooling convolution
-struct mma_pool_s {
-	uint32_t          gp:1; // global pooling {0}
-	uint32_t          pm:1; // pooling mode {0}, Max=0, AVG=1
-	uint32_t          cm:1; // ceil mode {0}, ceil=0, floor=1
-	uint32_t          ex:1; // exclusive {1}, if ignore padding in avg pooling
-	uint32_t   reserved:28; // reserved {0}
-	int32_t*            ia; // input address, INT32[N,Ci,Hi,Wi]
-	int32_t*            oa; // output address, INT32[N,Ci,Ho,Wo]
-	struct mma_input_s  i; // input
-	struct mma_kernel_s k; // kernel
-	struct mma_output_s o; // output
-};
-
-// Full connection
-struct mma_fcon_s {
-	uint32_t at; // activation type {0}, None=0, RELU=1
-	int8_t * ia; // input address, INT8[M,K]
-	int8_t * ka; // kernel address, INT8[K,N]
-	int32_t* ba; // bias address, INT32[M,N]
-	int32_t* oa; // output address, INT32[M,N] = ia[M,K] * wa[K,N] + ba[M,N]
-	int m, n, k; // dims
-};
-
-// Regisger access
-struct mma_creg_s {
-	uint32_t addr;
-	uint32_t data;
-};
-
-#define MMA_MAGIC_ID (('A' + 'L' + 'T' + 'R') / 4)
-
-/* Ioctls */
-#define MMA_IOCTL_MAKE(cmd)  ( _IO( MMA_MAGIC_ID, cmd))
-#define MMA_IOCTL_GET(cmd)   ( _IOC_NR(cmd))
-#define MMA_IOCTL_VALID(cmd) ((_IOC_TYPE(cmd)==MMA_MAGIC_ID) ? 1 : 0)
-
-#define MMA_CMD_INFO      0x00 // struct mma_info_s
-#define MMA_CMD_RESET     0x01 // struct mma_reset_s
-
-#define MMA_CMD_MCOPY     0x10 // struct mma_mcopy_s
-#define MMA_CMD_INVAL     0x11 // struct mma_cache_s
-#define MMA_CMD_FLUSH     0x12 // struct mma_cache_s
-
-#define MMA_CMD_CONV      0x20 // struct mma_conv_s
-#define MMA_CMD_POOL      0x21 // struct mma_pool_s
-#define MMA_CMD_FCON      0x22 // struct mma_fcon_s
-
-#define MMA_CMD_REGRD     0xC0 // struct mma_register_s
-#define MMA_CMD_REGWR     0xC1 // struct mma_register_s
-
-//---------------------------------------------------------------------------
-
-// device open/close
-int  mma_open();
-void mma_close();
-
-void mma_reset(struct mma_reset_s& args);
-
-// memory management
-void* mma_malloc(size_t size);
-void  mma_free(void* ptr);
-
-void* mma_mbias(size_t size);
-void* mma_mscale(size_t size);
-void* mma_minput(size_t size);
-void* mma_mkernel(size_t size);
-void* mma_moutput(size_t size);
-
-void mma_copy(void* dst, void* src, int size);
-int  mma_flush(void* addr, size_t size);
-int  mma_invalidate(void* addr, size_t size);
-
-// device information
-int mma_info(struct mma_info_s& args);
-
-// convolution process
-int mma_conv(struct mma_conv_s& args);
-int mma_pooling(struct mma_pool_s& args);
-int mma_fullconnect(struct mma_fcon_s& args);
-
-}  // namespace mma
-}  // namespace lite
-}  // namespace paddle
-
-#endif  // _LLDRV_MMA_H_
diff --git a/lite/backends/mma/lldrv/utils.cpp b/lite/backends/mma/lldrv/utils.cpp
deleted file mode 100755
index 92ed3cb..0000000
--- a/lite/backends/mma/lldrv/utils.cpp
+++ /dev/null
@@ -1,80 +0,0 @@
-/* Copyright (c) 2020 AWCloud. All Rights Reserved.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License. */
-
-#include <memory.h>
-#include <algorithm>
-#include <fstream>
-#include <string>
-
-#include "lite/backends/mma/lldrv/utils.h"
-
-namespace paddle {
-namespace lite {
-namespace mma {
-
-//---------------------------------------------------------------------------
-
-float find_max(const float* data, int size) 
-{
-	float max = 0.0;
-	
-	for (size_t i=0; i<size; ++i) {
-		float value = data[i];
-		float abs = value>0.0 ? value : -value;
-		
-		max = std::max(max, abs);
-	}
-	
-	return max;
-}
-
-void quantize_s8(const float* src, int8_t* dst, int size, float factor) 
-{
-	float fdata;
-	
-	for (size_t i=0; i<size; i++) {
-		fdata = src[i] * factor;
-		
-		if (fdata<0.0) {
-			fdata -= 0.5;
-		} else {
-			fdata += 0.5;
-		}
-		
-		dst[i] = (int8_t)fdata;
-	}
-}
-
-void quantize_s32(const float* src, int32_t* dst, int size, float factor) 
-{
-	float fdata;
-	
-	for (size_t i=0; i<size; i++) {
-		fdata = src[i] * factor;
-		
-		if (fdata<0.0) {
-			fdata -= 0.5;
-		} else {
-			fdata += 0.5;
-		}
-		
-		dst[i] = (int32_t)fdata;
-	}
-}
-
-//---------------------------------------------------------------------------
-
-}  // namespace mma
-}  // namespace lite
-}  // namespace paddle
diff --git a/lite/backends/mma/lldrv/utils.h b/lite/backends/mma/lldrv/utils.h
deleted file mode 100644
index 21f8712..0000000
--- a/lite/backends/mma/lldrv/utils.h
+++ /dev/null
@@ -1,43 +0,0 @@
-/* Copyright (c) 2020 AWCloud. All Rights Reserved.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License. */
-
-#pragma once
-
-#include <cstdint>
-#include <cstdlib>
-#include <cwchar>
-#include <vector>
-#include <sys/time.h>
-
-namespace paddle {
-namespace lite {
-namespace mma {
-
-inline int64_t get_tminus( void ) 
-{
-	struct timeval time;
-	
-	gettimeofday(&time, NULL);
-	
-	return 1000000LL * (int64_t)time.tv_sec + (int64_t)time.tv_usec;
-}
-
-float find_max(const float* data, int size);
-
-void quantize_s8 (const float* src, int8_t* dst, int size, float factor);
-void quantize_s32(const float* src, int32_t* dst, int size, float factor);
-
-}  // namespace mma
-}  // namespace lite
-}  // namespace paddle
diff --git a/lite/backends/mma/target_wrapper.cpp b/lite/backends/mma/target_wrapper.cpp
deleted file mode 100644
index 8ba95e8..0000000
--- a/lite/backends/mma/target_wrapper.cpp
+++ /dev/null
@@ -1,38 +0,0 @@
-// Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-#include "lite/utils/all.h"
-#include "lite/backends/mma/target_wrapper.h"
-#include "lite/backends/mma/lldrv/mmadrv.h"
-
-namespace paddle {
-namespace lite {
-
-void* TargetWrapper<TARGET(kMMA)>::Malloc(size_t size) {
-	return mma::mma_malloc(size);
-}
-
-void TargetWrapper<TARGET(kMMA)>::Free(void* ptr) {
-	mma::mma_free(ptr); 
-}
-
-void TargetWrapper<TARGET(kMMA)>::MemcpySync(void* dst,
-                                              const void* src,
-                                              size_t size,
-                                              IoDirection dir) {
-  memcpy(dst, src, size);
-}
-
-}  // namespace lite
-}  // namespace paddle
diff --git a/lite/backends/mma/target_wrapper.h b/lite/backends/mma/target_wrapper.h
deleted file mode 100644
index f2f5940..0000000
--- a/lite/backends/mma/target_wrapper.h
+++ /dev/null
@@ -1,60 +0,0 @@
-// Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-#pragma once
-
-#include <map>
-#include "lite/core/target_wrapper.h"
-
-namespace paddle {
-namespace lite {
-
-template <>
-class TargetWrapper<TARGET(kMMA)> {
- public:
-  using stream_t = int;
-  using event_t = int;
-
-  static size_t num_devices() { return 0; }
-  static size_t maximum_stream() { return 0; }
-
-  static void CreateStream(stream_t* stream) {}
-  static void DestroyStream(const stream_t& stream) {}
-
-  static void CreateEvent(event_t* event) {}
-  static void DestroyEvent(const event_t& event) {}
-
-  static void RecordEvent(const event_t& event) {}
-  static void SyncEvent(const event_t& event) {}
-
-  static void StreamSync(const stream_t& stream) {}
-
-  static void* Malloc(size_t size);
-  static void Free(void* ptr);
-
-  static void MemcpySync(void* dst,
-                         const void* src,
-                         size_t size,
-                         IoDirection dir);
-  static void MemcpyAsync(void* dst,
-                          const void* src,
-                          size_t size,
-                          IoDirection dir,
-                          const stream_t& stream) {
-    MemcpySync(dst, src, size, dir);
-  }
-};
-
-}  // namespace lite
-}  // namespace paddle
diff --git a/lite/core/CMakeLists.txt b/lite/core/CMakeLists.txt
index ef62888..9e03ca6 100644
--- a/lite/core/CMakeLists.txt
+++ b/lite/core/CMakeLists.txt
@@ -8,7 +8,6 @@ lite_cc_library(target_wrapper SRCS target_wrapper.cc
   XPU_DEPS target_wrapper_xpu
   CL_DEPS cl_target_wrapper
   FPGA_DEPS fpga_target_wrapper
-  MMA_DEPS mma_target_wrapper
   BM_DEPS target_wrapper_bm
   MLU_DEPS target_wrapper_mlu)
 
diff --git a/lite/core/context.h b/lite/core/context.h
index c03a8aa..dca559f 100644
--- a/lite/core/context.h
+++ b/lite/core/context.h
@@ -60,7 +60,6 @@ using APUContext = Context<TargetType::kAPU>;
 using XPUContext = Context<TargetType::kXPU>;
 using OpenCLContext = Context<TargetType::kOpenCL>;
 using FPGAContext = Context<TargetType::kFPGA>;
-using MMAContext = Context<TargetType::kMMA>;
 using BMContext = Context<TargetType::kBM>;
 using MLUContext = Context<TargetType::kMLU>;
 using RKNPUContext = Context<TargetType::kRKNPU>;
@@ -328,20 +327,6 @@ class Context<TargetType::kFPGA> {
 };
 #endif
 
-#ifdef LITE_WITH_MMA
-template <>
-class Context<TargetType::kMMA> {
- public:
-  void InitOnce() {}
-
-  MMAContext& operator=(const MMAContext& ctx) {}
-
-  void CopySharedTo(MMAContext* ctx) {}
-
-  std::string name() const { return "MMAContext"; }
-};
-#endif
-
 #ifdef LITE_WITH_MLU
 template <>
 class Context<TargetType::kMLU> {
@@ -562,12 +547,6 @@ class ContextScheduler {
             &ctx->As<FPGAContext>());
         break;
 #endif
-#ifdef LITE_WITH_MMA
-      case TARGET(kMMA):
-        kernel_contexts_[TargetType::kMMA].As<MMAContext>().CopySharedTo(
-            &ctx->As<MMAContext>());
-        break;
-#endif
 #ifdef LITE_WITH_BM
       case TARGET(kBM):
         kernel_contexts_[TargetType::kBM].As<BMContext>().CopySharedTo(
@@ -623,9 +602,6 @@ class ContextScheduler {
 #ifdef LITE_WITH_FPGA
     InitContext<TargetType::kFPGA, FPGAContext>();
 #endif
-#ifdef LITE_WITH_MMA
-    InitContext<TargetType::kMMA, MMAContext>();
-#endif
 #ifdef LITE_WITH_NPU
     InitContext<TargetType::kNPU, NPUContext>();
 #endif
diff --git a/lite/gen_code/CMakeLists.txt b/lite/gen_code/CMakeLists.txt
index 5ee22ae..06e6c00 100644
--- a/lite/gen_code/CMakeLists.txt
+++ b/lite/gen_code/CMakeLists.txt
@@ -21,7 +21,6 @@ lite_cc_test(test_gen_code SRCS gen_code_test.cc
         XPU_DEPS ${xpu_kernels}
         CL_DEPS ${opencl_kernels}
         FPGA_DEPS ${fpga_kernels}
-        MMA_DEPS ${mma_kernels}
         EXCLUDE_COMPILE_DEPS "ON"
         ARGS --optimized_model=${LITE_MODEL_DIR}/lite_naive_model_opt SERIAL)
 
@@ -53,7 +52,6 @@ lite_cc_test(test_generated_code SRCS generated_code_test.cc DEPS __generated_co
     XPU_DEPS ${xpu_kernels}
     CL_DEPS ${opencl_kernels}
     FPGA_DEPS ${fpga_kernels}
-    MMA_DEPS ${mma_kernels}
     EXCLUDE_COMPILE_DEPS "ON"
 )
  
diff --git a/lite/kernels/CMakeLists.txt b/lite/kernels/CMakeLists.txt
index 2fdd2d4..79cce9a 100644
--- a/lite/kernels/CMakeLists.txt
+++ b/lite/kernels/CMakeLists.txt
@@ -16,4 +16,3 @@ add_subdirectory(bm)
 add_subdirectory(rknpu)
 add_subdirectory(huawei_ascend_npu)
 add_subdirectory(imagination_nna)
-add_subdirectory(mma)
diff --git a/lite/kernels/mma/CMakeLists.txt b/lite/kernels/mma/CMakeLists.txt
deleted file mode 100755
index 5abdcf2..0000000
--- a/lite/kernels/mma/CMakeLists.txt
+++ /dev/null
@@ -1,10 +0,0 @@
-if ((NOT LITE_ON_MODEL_OPTIMIZE_TOOL) AND (NOT LITE_WITH_PYTHON) AND (NOT LITE_WITH_MMA))
-    return()
-endif()
-
-set(mma_deps mma_target_wrapper kernel_mma)
-
-add_kernel(conv_depthwise_mma MMA basic SRCS conv_depthwise.cc DEPS ${mma_deps})
-add_kernel(conv_gemmlike_mma MMA basic SRCS conv_gemmlike.cc DEPS ${mma_deps})
-add_kernel(conv_compute_mma MMA basic SRCS conv_compute.cc DEPS ${mma_deps} conv_depthwise_mma conv_gemmlike_mma)
-
diff --git a/lite/kernels/mma/conv_compute.cc b/lite/kernels/mma/conv_compute.cc
deleted file mode 100644
index ca50156..0000000
--- a/lite/kernels/mma/conv_compute.cc
+++ /dev/null
@@ -1,97 +0,0 @@
-// Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-#include <utility>
-#include "lite/core/op_registry.h"
-#include "lite/core/type_system.h"
-#include "lite/kernels/mma/conv_compute.h"
-#include "lite/kernels/mma/conv_depthwise.h"
-#include "lite/kernels/mma/conv_gemmlike.h"
-
-namespace paddle {
-namespace lite {
-namespace kernels {
-namespace mma {
-#define PARAM_INIT                                                           \
-  auto& param = this->Param<param_t>();                                      \
-  auto w_dims = param.filter->dims();                                        \
-  auto paddings = *param.paddings;                                           \
-  auto dilations = *param.dilations;                                         \
-  int ic = w_dims[1] * param.groups;                                         \
-  int oc = w_dims[0];                                                        \
-  int kh = w_dims[2];                                                        \
-  int kw = w_dims[3];                                                        \
-  int pad_h = paddings[0];                                                   \
-  int pad_w = paddings[2];                                                   \
-  int stride = param.strides[0];                                             \
-  int sh = param.strides[1];                                                 \
-  int sw = param.strides[0];                                                 \
-  int chin = param.x->dims()[1];                                             \
-  int hin = param.x->dims()[2];                                              \
-  int win = param.x->dims()[3];                                              \
-  int chout = param.output->dims()[1];                                       \
-  int hout = param.output->dims()[2];                                        \
-  int wout = param.output->dims()[3];                                        \
-  bool pads_equal =                                                          \
-      ((paddings[0] == paddings[1]) && (paddings[2] == paddings[3]));        \
-  bool pads_all_equal = (pads_equal && pad_h == pad_w);                      \
-  bool ks_equal = (sw == sh) && (kw == kh);                                  \
-  bool no_dilation = (dilations[0] == 1) && (dilations[1] == 1);             \
-  bool kps_equal = (pad_h == pad_w) && ks_equal;                             \
-  bool flag_dw_3x3 = (kw == 3) && (kh == 3) && (stride == 1 || stride == 2); \
-  bool flag_dw_5x5 = (kw == 5) && (kh == 5) && (stride == 1 || stride == 2); \
-  bool flag_dw = flag_dw_3x3 || flag_dw_5x5;
-
-template <>
-void ConvCompute<PRECISION(kFloat), PRECISION(kFloat)>::PrepareForRun() {
-  PARAM_INIT
-  /// select conv impl
-  if (param.groups == ic && ic == oc && ks_equal && no_dilation && flag_dw) {
-    impl_ = new DepthwiseConv<PRECISION(kFloat), PRECISION(kFloat)>;
-    // VLOG(3) << "invoking dw conv";
-  } else {
-    impl_ = new GemmLikeConv<PRECISION(kFloat), PRECISION(kFloat)>;
-    // VLOG(3) << "invoking gemm like conv";
-  }
-  if (!arm_cxt_) {
-  	arm_cxt_ = ContextScheduler::Global().NewContext(TargetType::kARM);
-  }
-  impl_->SetContext(std::move(arm_cxt_));
-  impl_->SetParam(param);
-  impl_->PrepareForRun();
-  is_first_epoch_ = false;
-}
-
-}  // namespace mma
-}  // namespace kernels
-}  // namespace lite
-}  // namespace paddle
-
-typedef paddle::lite::kernels::mma::ConvCompute<PRECISION(kFloat), PRECISION(kFloat)> ConvFp32;
-
-REGISTER_LITE_KERNEL(conv2d, kMMA, kFloat, kNCHW, ConvFp32, def)
-    .BindInput("Input", {LiteType::GetTensorTy(TARGET(kARM))})
-    .BindInput("Bias", {LiteType::GetTensorTy(TARGET(kARM))})
-    .BindInput("Filter", {LiteType::GetTensorTy(TARGET(kARM))})
-    .BindOutput("Output", {LiteType::GetTensorTy(TARGET(kARM))})
-    .BindPaddleOpVersion("conv2d", 1)
-    .Finalize();
-
-REGISTER_LITE_KERNEL(depthwise_conv2d, kMMA, kFloat, kNCHW, ConvFp32, def)
-    .BindInput("Input", {LiteType::GetTensorTy(TARGET(kARM))})
-    .BindInput("Bias", {LiteType::GetTensorTy(TARGET(kARM))})
-    .BindInput("Filter", {LiteType::GetTensorTy(TARGET(kARM))})
-    .BindOutput("Output", {LiteType::GetTensorTy(TARGET(kARM))})
-    .BindPaddleOpVersion("depthwise_conv2d", 1)
-    .Finalize();
diff --git a/lite/kernels/mma/conv_compute.h b/lite/kernels/mma/conv_compute.h
deleted file mode 100644
index 4385d49..0000000
--- a/lite/kernels/mma/conv_compute.h
+++ /dev/null
@@ -1,54 +0,0 @@
-// Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-#pragma once
-#include "lite/backends/arm/math/funcs.h"
-#include "lite/core/kernel.h"
-
-namespace paddle {
-namespace lite {
-namespace kernels {
-namespace mma {
-
-template <PrecisionType Ptype, PrecisionType OutType>
-class ConvCompute : public KernelLite<TARGET(kMMA), Ptype> {
- public:
-  virtual void PrepareForRun();
-
-  virtual void ReInitWhenNeeded() {
-    CHECK(impl_);
-    impl_->ReInitWhenNeeded();
-  }
-
-  virtual void Run() {
-    CHECK(impl_);
-    impl_->Run();
-  }
-
-  ~ConvCompute() {
-    if (impl_ != nullptr) {
-      delete impl_;
-    }
-  }
-
- private:
-  using param_t = operators::ConvParam;
-  std::unique_ptr<KernelContext> arm_cxt_{nullptr};
-  KernelLite<TARGET(kARM), Ptype>* impl_{nullptr};
-};
-
-}  // namespace mma
-}  // namespace kernels
-}  // namespace lite
-}  // namespace paddle
diff --git a/lite/kernels/mma/conv_depthwise.cc b/lite/kernels/mma/conv_depthwise.cc
deleted file mode 100644
index bad7f93..0000000
--- a/lite/kernels/mma/conv_depthwise.cc
+++ /dev/null
@@ -1,128 +0,0 @@
-// Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-#include "lite/backends/arm/math/conv_block_utils.h"
-#include "lite/backends/arm/math/conv_impl.h"
-#include "lite/kernels/mma/conv_depthwise.h"
-
-namespace paddle {
-namespace lite {
-namespace kernels {
-namespace mma {
-
-template <>
-void DepthwiseConv<PRECISION(kFloat), PRECISION(kFloat)>::ReInitWhenNeeded() {}
-
-template <>
-void DepthwiseConv<PRECISION(kFloat), PRECISION(kFloat)>::PrepareForRun() {
-  auto& param = this->Param<param_t>();
-  CHECK(this->ctx_);
-  auto& ctx = this->ctx_->template As<ARMContext>();
-  auto w_dims = param.filter->dims();
-  auto kw = w_dims[3];
-  auto channel = w_dims[0];
-  auto hin = param.x->dims()[2];
-  auto win = param.x->dims()[3];
-  auto paddings = *param.paddings;
-  // select dw conv kernel
-  if (kw == 3) {
-    bool pads_less = ((paddings[1] < 2) && (paddings[3] < 2));
-    if (pads_less && paddings[0] == paddings[2] &&
-        (paddings[0] == 0 || paddings[0] == 1)) {
-      flag_trans_weights_ = false;
-    } else {
-      // trans weights
-      constexpr int cblock = 4;
-      auto oc = w_dims[0];
-      auto kh = w_dims[2];
-      auto cround = ROUNDUP(oc, cblock);
-      weights_.Resize({cround, 1, kh, kw});
-      auto w_data = weights_.mutable_data<float>();
-      auto w_data_in = param.filter->data<float>();
-      lite::arm::math::conv_trans_weights_numc(
-          w_data_in, w_data, oc, 1, cblock, kh * kw);
-      flag_trans_weights_ = true;
-    }
-    impl_ = lite::arm::math::conv_depthwise_3x3_fp32;
-  } else if (kw == 5) {
-    auto strides = param.strides;
-    if ((strides[0] == 1 && strides[1] == 1) ||
-        (strides[0] == 2 && strides[1] == 2)) {
-      // trans weights
-      constexpr int cblock = 4;
-      auto oc = w_dims[0];
-      auto kh = w_dims[2];
-      auto cround = ROUNDUP(oc, cblock);
-      weights_.Resize({cround, 1, kh, kw});
-      auto w_data = weights_.mutable_data<float>();
-      auto w_data_in = param.filter->data<float>();
-      lite::arm::math::conv_trans_weights_numc(
-          w_data_in, w_data, oc, 1, cblock, kh * kw);
-      flag_trans_weights_ = true;
-      impl_ = lite::arm::math::conv_depthwise_5x5_fp32;
-    } else {
-      LOG(FATAL)
-          << "5x5 depthwise conv only support stride == 1 or stride == 2";
-    }
-  } else {
-    LOG(FATAL) << "this type dw conv not impl";
-  }
-}
-
-template <>
-void DepthwiseConv<PRECISION(kFloat), PRECISION(kFloat)>::Run() {
-  auto& param = this->Param<param_t>();
-  CHECK(this->ctx_);
-  auto& ctx = this->ctx_->template As<ARMContext>();
-  const auto* i_data = param.x->data<float>();
-  const auto* w_data = flag_trans_weights_ ? weights_.data<float>()
-                                           : param.filter->data<float>();
-  const auto* b_data = param.bias ? param.bias->data<float>() : nullptr;
-  if (flag_trans_bias_) {
-    b_data = bias_.data<float>();
-  }
-  auto* o_data = param.output->mutable_data<float>();
-
-  auto x_dims = param.x->dims();
-  auto w_dims = param.filter->dims();
-  auto o_dims = param.output->dims();
-
-  int iw = x_dims[3];  // nchw
-  int ih = x_dims[2];
-  int ic = x_dims[1];
-  int bs = x_dims[0];
-  int oh = o_dims[2];
-  int ow = o_dims[3];
-  int oc = o_dims[1];
-
-  impl_(i_data,
-        o_data,
-        bs,
-        oc,
-        oh,
-        ow,
-        ic,
-        ih,
-        iw,
-        w_data,
-        b_data,
-        param,
-        &ctx,
-        w_scale_.data());
-}
-
-}  // namespace mma
-}  // namespace kernels
-}  // namespace lite
-}  // namespace paddle
diff --git a/lite/kernels/mma/conv_depthwise.h b/lite/kernels/mma/conv_depthwise.h
deleted file mode 100644
index 78ba6fd..0000000
--- a/lite/kernels/mma/conv_depthwise.h
+++ /dev/null
@@ -1,67 +0,0 @@
-// Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-#pragma once
-
-#include <cmath>
-#include <string>
-#include <vector>
-#include "lite/backends/arm/math/conv_impl.h"
-#include "lite/core/context.h"
-#include "lite/core/kernel.h"
-#include "lite/core/target_wrapper.h"
-
-namespace paddle {
-namespace lite {
-namespace kernels {
-namespace mma {
-
-template <PrecisionType Ptype, PrecisionType Otype>
-class DepthwiseConv : public KernelLite<TARGET(kARM), Ptype> {
- public:
-  typedef void (*conv_dw_impl)(const void* din,
-                               void* dout,
-                               int num,
-                               int ch_out,
-                               int h_out,
-                               int w_out,
-                               int ch_in,
-                               int h_in,
-                               int w_in,
-                               const void* weights,
-                               const float* bias,
-                               const operators::ConvParam& param,
-                               ARMContext* ctx,
-                               const float* scale);
-  DepthwiseConv() = default;
-  ~DepthwiseConv() {}
-  virtual void PrepareForRun();
-  virtual void ReInitWhenNeeded();
-  virtual void Run();
-
- private:
-  using param_t = operators::ConvParam;
-  Tensor weights_;
-  Tensor bias_;
-  DDim last_shape_;
-  bool flag_trans_weights_{false};
-  bool flag_trans_bias_{false};
-  conv_dw_impl impl_{nullptr};
-  std::vector<float> w_scale_;
-};
-
-}  // namespace mma
-}  // namespace kernels
-}  // namespace lite
-}  // namespace paddle
diff --git a/lite/kernels/mma/conv_gemmlike.cc b/lite/kernels/mma/conv_gemmlike.cc
deleted file mode 100644
index 58f973b..0000000
--- a/lite/kernels/mma/conv_gemmlike.cc
+++ /dev/null
@@ -1,168 +0,0 @@
-// Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-#include <vector>
-#include "lite/backends/arm/math/gemm_prepacked_int8.h"
-#include "lite/backends/arm/math/packed_sgemm.h"
-#include "lite/kernels/mma/conv_gemmlike.h"
-#include "lite/backends/mma/lldrv/utils.h"
-#include "lite/backends/mma/lldrv/mmadrv.h"
-
-namespace paddle {
-namespace lite {
-namespace kernels {
-namespace mma {
-
-template <>
-void GemmLikeConv<PRECISION(kFloat), PRECISION(kFloat)>::PrepareForRun() {
-  ReInitWhenNeeded();
-}
-
-template <>
-void GemmLikeConv<PRECISION(kFloat), PRECISION(kFloat)>::Run() {
-  auto& param = this->Param<param_t>();
-  auto& ctx = this->ctx_->template As<ARMContext>();
-  ctx.ExtendWorkspace(workspace_size_);
-  auto weights = param.filter->data<float>();
-  if (flag_trans_weights_) {
-    weights = weights_.data<float>();
-  }
-  const float* b_data = param.bias ? param.bias->data<float>() : nullptr;
-  if (flag_trans_bias_) {
-    b_data = bias_.data<float>();
-  }
-  auto i_data = param.x->data<float>();
-  auto w_data = param.filter->data<float>();
-  auto o_data = param.output->mutable_data<float>();
-  auto i_dims = param.x->dims();
-  auto w_dims = param.filter->dims();
-  auto o_dims = param.output->dims();
-  auto paddings = *param.paddings;
-  auto dilations = *param.dilations;
-
-  int iw, ih, ic, bs, ow, oh, oc;
-  float alpha;
-
-  iw = i_dims[3];  // nchw
-  ih = i_dims[2];
-  ic = i_dims[1];
-  bs = i_dims[0];
-  oh = o_dims[2];
-  ow = o_dims[3];
-  oc = o_dims[1];
-  
-  int kh = w_dims[2];
-  int kw = w_dims[3];
-/*  
-  std::cout << ">>> conv2d: I: " << bs << "x" << ic << "x" << iw << "x" << ih
-    << ", K: " << w_dims[0] << "x" << w_dims[1] << "x" << kw << "x" << kh 
-    << ", S: " << param.strides[1] << "x" << param.strides[0] 
-    << ", O: " << bs << "x" << oc << "x" << ow << "x" << oh << std::endl;
-*/    
-//------------------------------------------------------------------------------------
-	if (kh>1 && kw>1) {
-		int i, j, il, kl, ol, l, m, n, k;
-		lite::mma::mma_conv_s conv;
-		
-		conv.at = static_cast<uint32_t>(param.activation_param.active_type);
-		if (conv.at==4) {
-			alpha = param.activation_param.Leaky_relu_alpha;
-		}
-		conv.ng = param.groups;
-
-		conv.i.in = i_dims[0];
-		conv.i.ic = i_dims[1];
-		conv.i.ih = i_dims[2];
-		conv.i.iw = i_dims[3];
-		conv.i.pl = paddings[2]; // left
-		conv.i.pr = paddings[3]; // right
-		conv.i.pt = paddings[0]; // top
-		conv.i.pb = paddings[1]; // bottom
-		conv.i.dy = dilations[0];
-		conv.i.dx = dilations[1];
-
-		conv.k.kh = w_dims[2];
-		conv.k.kw = w_dims[3];
-		conv.k.hs = param.strides[0];
-		conv.k.ws = param.strides[1];
-
-		conv.o.on = o_dims[0];
-		conv.o.oc = o_dims[1];
-		conv.o.oh = o_dims[2];
-		conv.o.ow = o_dims[3];
-
-		il  = conv.i.in * conv.i.ic * conv.i.ih * conv.i.iw;
-		kl  = conv.o.oc * conv.i.ic * conv.k.kh * conv.k.kw;
-		ol  = conv.o.on * conv.o.oc * conv.o.oh * conv.o.ow;
-		conv.ia = static_cast<int8_t *>(lite::mma::mma_minput (il * sizeof(int8_t)));
-		conv.ka = static_cast<int8_t *>(lite::mma::mma_mkernel(kl * sizeof(int8_t)));
-		conv.oa = static_cast<int32_t*>(lite::mma::mma_moutput(ol * sizeof(int32_t)));
-		if (conv.ia && conv.ka && conv.oa) {
-			float fd = lite::mma::find_max(i_data, il);
-			float fw = lite::mma::find_max(w_data, kl);
-
-			fd = 127.0 / fd;
-			fw = 127.0 / fw;
-			
-			// y = 127.0 / fmax
-			// y = x * scale;
-			lite::mma::quantize_s8(i_data, conv.ia, il, fd);
-			lite::mma::quantize_s8(w_data, conv.ka, kl, fw);
-			
-			// perform conv2d
-			if (lite::mma::mma_conv(conv)) {
-				std::cout << "mma_conv error" << std::endl;
-			}
-			// Convert int32 back to fp32, [n,c,h,w]
-			// 1. y = x / scale
-			// 2. y = x + b
-			// 3. y = f(x)
-			int hw = conv.o.oh * conv.o.ow;
-			for (i=0; i<conv.o.on; i++) {
-				for (j=0; j<conv.o.oc; j++) {
-					m = i * conv.o.oc + j;
-					n = m * hw;
-					for (l=0; l<hw; l++) {
-						k = n+l;
-						o_data[k] = (float)conv.oa[k] / fd / fw;
-						if (b_data)
-							o_data[k] += b_data[j];
-						if (conv.at==1) {// relu
-							o_data[k] = o_data[k]>0.0 ? o_data[k] : 0.0;
-						} else if (conv.at==2) { // relu6
-							o_data[k] = o_data[k]>0.0 ? o_data[k] : 0.0;
-							o_data[k] = o_data[k]>6.0 ? 6.0 : o_data[k];
-						} else if (conv.at==4) { // leakyRelu
-							if (o_data[k]<0.0)
-								o_data[k] = o_data[k] * alpha;
-						}
-					}
-				}
-			}
-		}
-	} else {
-	  if (flag_1x1gemm_) {
-		lite::arm::math::conv1x1s1_gemm(
-		    i_data, o_data, bs, oc, oh, ow, ic, ih, iw, weights, b_data, param, &ctx);
-	  } else {
-		lite::arm::math::conv_im2col_gemm(
-		    i_data, o_data, bs, oc, oh, ow, ic, ih, iw, weights, b_data, param, &ctx);
-	  }
-  }
-}
-
-}  // namespace mma
-}  // namespace kernels
-}  // namespace lite
-}  // namespace paddle
diff --git a/lite/kernels/mma/conv_gemmlike.h b/lite/kernels/mma/conv_gemmlike.h
deleted file mode 100644
index 68fc72f..0000000
--- a/lite/kernels/mma/conv_gemmlike.h
+++ /dev/null
@@ -1,113 +0,0 @@
-// Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-#pragma once
-
-#include <cmath>
-#include <string>
-#include <vector>
-#include "lite/backends/arm/math/conv_impl.h"
-#include "lite/backends/arm/math/funcs.h"
-#include "lite/core/context.h"
-#include "lite/core/kernel.h"
-#include "lite/core/target_wrapper.h"
-
-namespace paddle {
-namespace lite {
-namespace kernels {
-namespace mma {
-
-template <PrecisionType Ptype, PrecisionType Otype>
-class GemmLikeConv : public KernelLite<TARGET(kARM), Ptype> {
- public:
-  GemmLikeConv() = default;
-  ~GemmLikeConv() {}
-
-  virtual void ReInitWhenNeeded() {
-    auto& param = this->template Param<param_t>();
-    CHECK(this->ctx_);
-    auto& ctx = this->ctx_->template As<ARMContext>();
-    auto x_dims = param.x->dims();
-    auto w_dims = param.filter->dims();
-    auto o_dims = param.output->dims();
-    if (last_shape_ == x_dims) {
-      return;
-    }
-
-    int iw = x_dims[3];  // nchw
-    int ih = x_dims[2];
-    int ic = x_dims[1];
-    int ow = o_dims[3];
-    int oh = o_dims[2];
-    int oc = o_dims[1];
-    int kw = w_dims[3];
-    int kh = w_dims[2];
-
-    auto paddings = *param.paddings;
-    auto dilations = *param.dilations;
-
-    int sw = param.strides[1];
-    int sh = param.strides[0];
-    int pw = paddings[2];
-    int ph = paddings[0];
-    int dw = dilations[1];
-    int dh = dilations[0];
-
-    bool pads_equal =
-        ((paddings[0] == paddings[1]) && (paddings[2] == paddings[3]));
-
-    int m = oc / param.groups;
-    int k = ic * kh * kw / param.groups;
-    int n = oh * ow;
-
-    bool kps_equal = (pw == ph) && (sw == sh) && (kw == kh);
-    bool ks_equal = (sw == sh) && (kw == kh);
-    //! select conv gemmlike kernel
-    if (kw == 1 && sw == 1 && pw == 0 && kps_equal && pads_equal) {
-      //! 1x1s1p0 gemmlike conv
-      flag_1x1gemm_ = true;
-    } else {
-      //! im2col gemmlike conv
-      flag_1x1gemm_ = false;
-      workspace_size_ = k * n * sizeof(float);
-    }
-    if (!flag_trans_weights_ && n > 1 && m > 1) {
-      lite::arm::math::trans_gemm_weights<Ptype>(
-          *(param.filter), weights_, param.groups, &ctx);
-      flag_trans_weights_ = true;
-    } else if (n == 1 || m == 1) {
-      flag_trans_weights_ = false;
-    }
-    last_shape_ = x_dims;
-  }
-  virtual void PrepareForRun();
-  virtual void Run();
-
-  /// todo, support inplace weights transform
- protected:
-  using param_t = operators::ConvParam;
-  DDim last_shape_;
-  std::vector<float> w_scale_;
-  bool flag_1x1gemm_{true};
-  bool flag_trans_weights_{false};
-  bool flag_trans_bias_{false};
-  Tensor weights_;
-  Tensor bias_;
-  int workspace_size_{0};
-};
-
-}  // namespace mma
-}  // namespace kernels
-}  // namespace lite
-}  // namespace paddle
